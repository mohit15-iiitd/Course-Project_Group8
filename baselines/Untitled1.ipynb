{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from autocorrect import Speller\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from transformers import TFBertTokenizer\n",
    "# from nlp import nlp\n",
    "import preprocessor as p\n",
    "import spacy\n",
    "from torch.utils.data import DataLoader\n",
    "import re\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import torch.nn as nn\n",
    "lemma = WordNetLemmatizer()\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, losses, models, util\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.readers import InputExample\n",
    "\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics.pairwise import paired_cosine_distances, paired_euclidean_distances, paired_manhattan_distances\n",
    "import fastText\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"uncleaned_combined_txt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>My autistic 3 year old said her first sentence...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>From having the left side of my lips almost co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>After dating for just over two years, I propos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>One day at a time. Smiling more makes a big di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>trying my hardest to be happy today. I hope my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>1</td>\n",
       "      <td>death sound appealingthought death thing bring...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>1</td>\n",
       "      <td>left relationship almost 5 years depression en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1761</th>\n",
       "      <td>1</td>\n",
       "      <td>always wanted friendlife never anyone loved ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1762</th>\n",
       "      <td>1</td>\n",
       "      <td>deal someone avoids everythinghi close friend ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>1</td>\n",
       "      <td>turns might depressedhey became orphan cry hon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1764 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0         0  My autistic 3 year old said her first sentence...\n",
       "1         0  From having the left side of my lips almost co...\n",
       "2         0  After dating for just over two years, I propos...\n",
       "3         0  One day at a time. Smiling more makes a big di...\n",
       "4         0  trying my hardest to be happy today. I hope my...\n",
       "...     ...                                                ...\n",
       "1759      1  death sound appealingthought death thing bring...\n",
       "1760      1  left relationship almost 5 years depression en...\n",
       "1761      1  always wanted friendlife never anyone loved ca...\n",
       "1762      1  deal someone avoids everythinghi close friend ...\n",
       "1763      1  turns might depressedhey became orphan cry hon...\n",
       "\n",
       "[1764 rows x 2 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing funciton created\n",
    "def preprocessing(df):\n",
    "    df1 = df.copy()\n",
    "    df1['white_space_removed'] = 0\n",
    "    df1['emoji_removed'] = 0\n",
    "    df1['tokenized_data'] = 0\n",
    "    df1['stopword_removed_data'] = 0\n",
    "    df1['punct_removed_data'] = 0\n",
    "    df1['url_removed_data'] = 0\n",
    "    df1['lemma_data'] = 0\n",
    "    df1['spelling_checked_data'] = 0\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "#   lemmatizer = WordNetLemmatizer()\n",
    "    spell = Speller(lang='en')\n",
    "\n",
    "\n",
    "  # iterate over each row of dataset and preprocess data\n",
    "    for i in range(df1.shape[0]):\n",
    "\n",
    "        # white space removel\n",
    "        df1['white_space_removed'][i] = re.sub(\"\\s+\", \" \", df1.text[i])\n",
    "        emoji_removed = re.sub(r'\\W+', ' ', df1['white_space_removed'][i].encode('ascii', 'ignore').decode('utf-8'), flags=re.UNICODE).strip()\n",
    "        df1['emoji_removed'] = emoji_removed\n",
    "\n",
    "        # lower casing and tokenization\n",
    "        lower = df1['emoji_removed'][i].lower()\n",
    "        tokenized_data = word_tokenize(lower)\n",
    "        df1['tokenized_data'][i] = tokenized_data\n",
    "        # print(tokenized_data)\n",
    "\n",
    "\n",
    "        # remove stopwords\n",
    "        stopword_removed_data = [x for x in tokenized_data if x not in stop_words]\n",
    "        df1['stopword_removed_data'][i] = stopword_removed_data \n",
    "        # print(stopword_removed_data)\n",
    "\n",
    "\n",
    "        # remove urls and html tags\n",
    "        urls = re.findall(\"https?://[a-zA-Z0-9_\\?=\\@\\/#=.~-]+\", \" \".join(stopword_removed_data))\n",
    "        url_removed_data = [x for x in stopword_removed_data if x not in urls]\n",
    "        df1['url_removed_data'][i] = url_removed_data\n",
    "        # print(url_removed_data)\n",
    "        # print(type(url_removed_data))\n",
    "        \n",
    "        \n",
    "        # punctuation removel\n",
    "        punct_removed_data = [x for x in url_removed_data if x.isalnum()]\n",
    "        df1['punct_removed_data'][i] = punct_removed_data\n",
    "        # print(punct_removed_data)\n",
    "        \n",
    "\n",
    "        spelling_checked_data = [spell(x) for x in punct_removed_data]\n",
    "        df1['spelling_checked_data'][i] = \" \".join(spelling_checked_data)\n",
    "#         print(\" \".join(spelling_checked_data))\n",
    "\n",
    "        lemma_data = [lemma.lemmatize(x) for x in spelling_checked_data]\n",
    "        df1['lemma_data'][i] = lemma_data\n",
    "        # spelling checking\n",
    "\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label                                                    0\n",
       "text     My autistic 3 year old said her first sentence...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = preprocessing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>white_space_removed</th>\n",
       "      <th>emoji_removed</th>\n",
       "      <th>tokenized_data</th>\n",
       "      <th>stopword_removed_data</th>\n",
       "      <th>punct_removed_data</th>\n",
       "      <th>url_removed_data</th>\n",
       "      <th>lemma_data</th>\n",
       "      <th>spelling_checked_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>My autistic 3 year old said her first sentence...</td>\n",
       "      <td>My autistic 3 year old said her first sentence...</td>\n",
       "      <td>turns might depressedhey became orphan cry hon...</td>\n",
       "      <td>[my, autistic, 3, year, old, said, her, first,...</td>\n",
       "      <td>[autistic, 3, year, old, said, first, sentence...</td>\n",
       "      <td>[autistic, 3, year, old, said, first, sentence...</td>\n",
       "      <td>[autistic, 3, year, old, said, first, sentence...</td>\n",
       "      <td>[autistic, 3, year, old, said, first, sentence...</td>\n",
       "      <td>autistic 3 year old said first sentence today ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>From having the left side of my lips almost co...</td>\n",
       "      <td>From having the left side of my lips almost co...</td>\n",
       "      <td>turns might depressedhey became orphan cry hon...</td>\n",
       "      <td>[from, having, the, left, side, of, my, lips, ...</td>\n",
       "      <td>[left, side, lips, almost, completely, torn, f...</td>\n",
       "      <td>[left, side, lips, almost, completely, torn, f...</td>\n",
       "      <td>[left, side, lips, almost, completely, torn, f...</td>\n",
       "      <td>[left, side, lip, almost, completely, torn, fa...</td>\n",
       "      <td>left side lips almost completely torn face dog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>After dating for just over two years, I propos...</td>\n",
       "      <td>After dating for just over two years, I propos...</td>\n",
       "      <td>turns might depressedhey became orphan cry hon...</td>\n",
       "      <td>[after, dating, for, just, over, two, years, i...</td>\n",
       "      <td>[dating, two, years, proposed, said, yes, enga...</td>\n",
       "      <td>[dating, two, years, proposed, said, yes, enga...</td>\n",
       "      <td>[dating, two, years, proposed, said, yes, enga...</td>\n",
       "      <td>[dating, two, year, proposed, said, yes, engaged]</td>\n",
       "      <td>dating two years proposed said yes engaged</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>One day at a time. Smiling more makes a big di...</td>\n",
       "      <td>One day at a time. Smiling more makes a big di...</td>\n",
       "      <td>turns might depressedhey became orphan cry hon...</td>\n",
       "      <td>[one, day, at, a, time, smiling, more, makes, ...</td>\n",
       "      <td>[one, day, time, smiling, makes, big, difference]</td>\n",
       "      <td>[one, day, time, smiling, makes, big, difference]</td>\n",
       "      <td>[one, day, time, smiling, makes, big, difference]</td>\n",
       "      <td>[one, day, time, smiling, make, big, difference]</td>\n",
       "      <td>one day time smiling makes big difference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>trying my hardest to be happy today. I hope my...</td>\n",
       "      <td>trying my hardest to be happy today. I hope my...</td>\n",
       "      <td>turns might depressedhey became orphan cry hon...</td>\n",
       "      <td>[trying, my, hardest, to, be, happy, today, i,...</td>\n",
       "      <td>[trying, hardest, happy, today, hope, face, br...</td>\n",
       "      <td>[trying, hardest, happy, today, hope, face, br...</td>\n",
       "      <td>[trying, hardest, happy, today, hope, face, br...</td>\n",
       "      <td>[trying, hardest, happy, today, hope, face, br...</td>\n",
       "      <td>trying hardest happy today hope face brings so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>1</td>\n",
       "      <td>death sound appealingthought death thing bring...</td>\n",
       "      <td>death sound appealingthought death thing bring...</td>\n",
       "      <td>turns might depressedhey became orphan cry hon...</td>\n",
       "      <td>[death, sound, appealingthought, death, thing,...</td>\n",
       "      <td>[death, sound, appealingthought, death, thing,...</td>\n",
       "      <td>[death, sound, appealingthought, death, thing,...</td>\n",
       "      <td>[death, sound, appealingthought, death, thing,...</td>\n",
       "      <td>[death, sound, appealingthought, death, thing,...</td>\n",
       "      <td>death sound appealingthought death thing bring...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>1</td>\n",
       "      <td>left relationship almost 5 years depression en...</td>\n",
       "      <td>left relationship almost 5 years depression en...</td>\n",
       "      <td>turns might depressedhey became orphan cry hon...</td>\n",
       "      <td>[left, relationship, almost, 5, years, depress...</td>\n",
       "      <td>[left, relationship, almost, 5, years, depress...</td>\n",
       "      <td>[left, relationship, almost, 5, years, depress...</td>\n",
       "      <td>[left, relationship, almost, 5, years, depress...</td>\n",
       "      <td>[left, relationship, almost, 5, year, depressi...</td>\n",
       "      <td>left relationship almost 5 years depression en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1761</th>\n",
       "      <td>1</td>\n",
       "      <td>always wanted friendlife never anyone loved ca...</td>\n",
       "      <td>always wanted friendlife never anyone loved ca...</td>\n",
       "      <td>turns might depressedhey became orphan cry hon...</td>\n",
       "      <td>[always, wanted, friendlife, never, anyone, lo...</td>\n",
       "      <td>[always, wanted, friendlife, never, anyone, lo...</td>\n",
       "      <td>[always, wanted, friendlife, never, anyone, lo...</td>\n",
       "      <td>[always, wanted, friendlife, never, anyone, lo...</td>\n",
       "      <td>[always, wanted, friendly, never, anyone, love...</td>\n",
       "      <td>always wanted friendlies never anyone loved ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1762</th>\n",
       "      <td>1</td>\n",
       "      <td>deal someone avoids everythinghi close friend ...</td>\n",
       "      <td>deal someone avoids everythinghi close friend ...</td>\n",
       "      <td>turns might depressedhey became orphan cry hon...</td>\n",
       "      <td>[deal, someone, avoids, everythinghi, close, f...</td>\n",
       "      <td>[deal, someone, avoids, everythinghi, close, f...</td>\n",
       "      <td>[deal, someone, avoids, everythinghi, close, f...</td>\n",
       "      <td>[deal, someone, avoids, everythinghi, close, f...</td>\n",
       "      <td>[deal, someone, avoids, everything, close, fri...</td>\n",
       "      <td>deal someone avoids everything close friend le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>1</td>\n",
       "      <td>turns might depressedhey became orphan cry hon...</td>\n",
       "      <td>turns might depressedhey became orphan cry hon...</td>\n",
       "      <td>turns might depressedhey became orphan cry hon...</td>\n",
       "      <td>[turns, might, depressedhey, became, orphan, c...</td>\n",
       "      <td>[turns, might, depressedhey, became, orphan, c...</td>\n",
       "      <td>[turns, might, depressedhey, became, orphan, c...</td>\n",
       "      <td>[turns, might, depressedhey, became, orphan, c...</td>\n",
       "      <td>[turn, might, depressedhey, became, orphan, cr...</td>\n",
       "      <td>turns might depressedhey became orphan cry hon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1764 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  \\\n",
       "0         0  My autistic 3 year old said her first sentence...   \n",
       "1         0  From having the left side of my lips almost co...   \n",
       "2         0  After dating for just over two years, I propos...   \n",
       "3         0  One day at a time. Smiling more makes a big di...   \n",
       "4         0  trying my hardest to be happy today. I hope my...   \n",
       "...     ...                                                ...   \n",
       "1759      1  death sound appealingthought death thing bring...   \n",
       "1760      1  left relationship almost 5 years depression en...   \n",
       "1761      1  always wanted friendlife never anyone loved ca...   \n",
       "1762      1  deal someone avoids everythinghi close friend ...   \n",
       "1763      1  turns might depressedhey became orphan cry hon...   \n",
       "\n",
       "                                    white_space_removed  \\\n",
       "0     My autistic 3 year old said her first sentence...   \n",
       "1     From having the left side of my lips almost co...   \n",
       "2     After dating for just over two years, I propos...   \n",
       "3     One day at a time. Smiling more makes a big di...   \n",
       "4     trying my hardest to be happy today. I hope my...   \n",
       "...                                                 ...   \n",
       "1759  death sound appealingthought death thing bring...   \n",
       "1760  left relationship almost 5 years depression en...   \n",
       "1761  always wanted friendlife never anyone loved ca...   \n",
       "1762  deal someone avoids everythinghi close friend ...   \n",
       "1763  turns might depressedhey became orphan cry hon...   \n",
       "\n",
       "                                          emoji_removed  \\\n",
       "0     turns might depressedhey became orphan cry hon...   \n",
       "1     turns might depressedhey became orphan cry hon...   \n",
       "2     turns might depressedhey became orphan cry hon...   \n",
       "3     turns might depressedhey became orphan cry hon...   \n",
       "4     turns might depressedhey became orphan cry hon...   \n",
       "...                                                 ...   \n",
       "1759  turns might depressedhey became orphan cry hon...   \n",
       "1760  turns might depressedhey became orphan cry hon...   \n",
       "1761  turns might depressedhey became orphan cry hon...   \n",
       "1762  turns might depressedhey became orphan cry hon...   \n",
       "1763  turns might depressedhey became orphan cry hon...   \n",
       "\n",
       "                                         tokenized_data  \\\n",
       "0     [my, autistic, 3, year, old, said, her, first,...   \n",
       "1     [from, having, the, left, side, of, my, lips, ...   \n",
       "2     [after, dating, for, just, over, two, years, i...   \n",
       "3     [one, day, at, a, time, smiling, more, makes, ...   \n",
       "4     [trying, my, hardest, to, be, happy, today, i,...   \n",
       "...                                                 ...   \n",
       "1759  [death, sound, appealingthought, death, thing,...   \n",
       "1760  [left, relationship, almost, 5, years, depress...   \n",
       "1761  [always, wanted, friendlife, never, anyone, lo...   \n",
       "1762  [deal, someone, avoids, everythinghi, close, f...   \n",
       "1763  [turns, might, depressedhey, became, orphan, c...   \n",
       "\n",
       "                                  stopword_removed_data  \\\n",
       "0     [autistic, 3, year, old, said, first, sentence...   \n",
       "1     [left, side, lips, almost, completely, torn, f...   \n",
       "2     [dating, two, years, proposed, said, yes, enga...   \n",
       "3     [one, day, time, smiling, makes, big, difference]   \n",
       "4     [trying, hardest, happy, today, hope, face, br...   \n",
       "...                                                 ...   \n",
       "1759  [death, sound, appealingthought, death, thing,...   \n",
       "1760  [left, relationship, almost, 5, years, depress...   \n",
       "1761  [always, wanted, friendlife, never, anyone, lo...   \n",
       "1762  [deal, someone, avoids, everythinghi, close, f...   \n",
       "1763  [turns, might, depressedhey, became, orphan, c...   \n",
       "\n",
       "                                     punct_removed_data  \\\n",
       "0     [autistic, 3, year, old, said, first, sentence...   \n",
       "1     [left, side, lips, almost, completely, torn, f...   \n",
       "2     [dating, two, years, proposed, said, yes, enga...   \n",
       "3     [one, day, time, smiling, makes, big, difference]   \n",
       "4     [trying, hardest, happy, today, hope, face, br...   \n",
       "...                                                 ...   \n",
       "1759  [death, sound, appealingthought, death, thing,...   \n",
       "1760  [left, relationship, almost, 5, years, depress...   \n",
       "1761  [always, wanted, friendlife, never, anyone, lo...   \n",
       "1762  [deal, someone, avoids, everythinghi, close, f...   \n",
       "1763  [turns, might, depressedhey, became, orphan, c...   \n",
       "\n",
       "                                       url_removed_data  \\\n",
       "0     [autistic, 3, year, old, said, first, sentence...   \n",
       "1     [left, side, lips, almost, completely, torn, f...   \n",
       "2     [dating, two, years, proposed, said, yes, enga...   \n",
       "3     [one, day, time, smiling, makes, big, difference]   \n",
       "4     [trying, hardest, happy, today, hope, face, br...   \n",
       "...                                                 ...   \n",
       "1759  [death, sound, appealingthought, death, thing,...   \n",
       "1760  [left, relationship, almost, 5, years, depress...   \n",
       "1761  [always, wanted, friendlife, never, anyone, lo...   \n",
       "1762  [deal, someone, avoids, everythinghi, close, f...   \n",
       "1763  [turns, might, depressedhey, became, orphan, c...   \n",
       "\n",
       "                                             lemma_data  \\\n",
       "0     [autistic, 3, year, old, said, first, sentence...   \n",
       "1     [left, side, lip, almost, completely, torn, fa...   \n",
       "2     [dating, two, year, proposed, said, yes, engaged]   \n",
       "3      [one, day, time, smiling, make, big, difference]   \n",
       "4     [trying, hardest, happy, today, hope, face, br...   \n",
       "...                                                 ...   \n",
       "1759  [death, sound, appealingthought, death, thing,...   \n",
       "1760  [left, relationship, almost, 5, year, depressi...   \n",
       "1761  [always, wanted, friendly, never, anyone, love...   \n",
       "1762  [deal, someone, avoids, everything, close, fri...   \n",
       "1763  [turn, might, depressedhey, became, orphan, cr...   \n",
       "\n",
       "                                  spelling_checked_data  \n",
       "0     autistic 3 year old said first sentence today ...  \n",
       "1     left side lips almost completely torn face dog...  \n",
       "2            dating two years proposed said yes engaged  \n",
       "3             one day time smiling makes big difference  \n",
       "4     trying hardest happy today hope face brings so...  \n",
       "...                                                 ...  \n",
       "1759  death sound appealingthought death thing bring...  \n",
       "1760  left relationship almost 5 years depression en...  \n",
       "1761  always wanted friendlies never anyone loved ca...  \n",
       "1762  deal someone avoids everything close friend le...  \n",
       "1763  turns might depressedhey became orphan cry hon...  \n",
       "\n",
       "[1764 rows x 10 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['text', 'target'],index = None)\n",
    "df['text'] = preprocessed_data['spelling_checked_data']\n",
    "df['target'] = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"cleaned_data_all.csv\", index = False)\n",
    "cleaned_df = pd.read_csv(\"cleaned_data_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"cleaned_data_all.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_cont_model = fastText.train_unsupervised(path, model='skipgram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_embedding = df.copy()\n",
    "for i in range(len(df)):\n",
    "    embeddings = np.array(non_cont_model.get_sentence_vector(df['text'][i]))\n",
    "    df_with_embedding['text'][i] = embeddings\n",
    "    df_with_embedding['target'][i] = df['target'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_embedding = df_with_embedding.rename(columns={'text':'Embeddings'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-0.039855443, 0.036473524, -0.20623395, 0.076...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.03649728, 0.033170026, -0.21308982, 0.0874...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.047610383, 0.030810263, -0.20189077, 0.061...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.03685748, 0.040161744, -0.21739762, 0.0895...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.027496582, 0.04259511, -0.22344425, 0.1079...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1759</th>\n",
       "      <td>[-0.020344233, 0.038744725, -0.21521005, 0.109...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1760</th>\n",
       "      <td>[-0.02823309, 0.035109952, -0.20898741, 0.0936...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1761</th>\n",
       "      <td>[-0.026655467, 0.036016535, -0.20715354, 0.091...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1762</th>\n",
       "      <td>[-0.027852943, 0.036258657, -0.21016794, 0.094...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>[-0.033368476, 0.035245802, -0.20841387, 0.085...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1764 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target\n",
       "0     [-0.039855443, 0.036473524, -0.20623395, 0.076...       0\n",
       "1     [-0.03649728, 0.033170026, -0.21308982, 0.0874...       0\n",
       "2     [-0.047610383, 0.030810263, -0.20189077, 0.061...       0\n",
       "3     [-0.03685748, 0.040161744, -0.21739762, 0.0895...       0\n",
       "4     [-0.027496582, 0.04259511, -0.22344425, 0.1079...       0\n",
       "...                                                 ...     ...\n",
       "1759  [-0.020344233, 0.038744725, -0.21521005, 0.109...       1\n",
       "1760  [-0.02823309, 0.035109952, -0.20898741, 0.0936...       1\n",
       "1761  [-0.026655467, 0.036016535, -0.20715354, 0.091...       1\n",
       "1762  [-0.027852943, 0.036258657, -0.21016794, 0.094...       1\n",
       "1763  [-0.033368476, 0.035245802, -0.20841387, 0.085...       1\n",
       "\n",
       "[1764 rows x 2 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_embedding.to_csv(\"clean_data_with_embeddings.csv\", index = False)\n",
    "cleaned_df = pd.read_csv(\"clean_data_with_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(df_with_embedding['Embeddings'], df_with_embedding['target'], train_size=0.2, random_state = 32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02511302,  0.03815467, -0.21342003,  0.10002109, -0.065419  ,\n",
       "       -0.07697617,  0.09253636,  0.11956444, -0.00414546, -0.05217087,\n",
       "       -0.14393789,  0.15659133, -0.03971355,  0.09986812, -0.04285796,\n",
       "       -0.00771237, -0.08103644,  0.0842102 , -0.13466698, -0.08008885,\n",
       "        0.06396492,  0.24965456, -0.17380159,  0.14163958,  0.0034283 ,\n",
       "       -0.13874602, -0.12467469, -0.01913846,  0.03838994,  0.0362975 ,\n",
       "       -0.01002271, -0.01973843, -0.10862789, -0.08652105,  0.00454351,\n",
       "        0.04948609,  0.01412504,  0.01680061,  0.00942602, -0.22177272,\n",
       "       -0.02632576, -0.02603904,  0.02229575, -0.07423657,  0.01040624,\n",
       "       -0.09878636, -0.05566059,  0.01560424,  0.04104643,  0.04644802,\n",
       "       -0.2168769 ,  0.14411113, -0.05867142,  0.09552859,  0.13457732,\n",
       "        0.11857323, -0.03419296, -0.00096229,  0.10469908,  0.14624128,\n",
       "        0.15130551,  0.13236019,  0.10106458, -0.01799941,  0.04789725,\n",
       "        0.12564701, -0.01646347,  0.09134252,  0.09216394, -0.0947082 ,\n",
       "       -0.09137942, -0.12215702,  0.11062185,  0.05826567, -0.16798773,\n",
       "        0.05191798,  0.04596918,  0.04010206,  0.10565238,  0.12257829,\n",
       "        0.06991347,  0.15538044,  0.11591419,  0.01780202, -0.00828369,\n",
       "       -0.14358906,  0.02461003,  0.00346225,  0.07107531,  0.25984412,\n",
       "       -0.05865434, -0.06314079,  0.04981874, -0.01957995,  0.08660121,\n",
       "        0.10656691, -0.00523632,  0.04148799, -0.09006875, -0.0814312 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=[]\n",
    "for i in range(len(train_x)):\n",
    "    embeddings.append(train_x.iloc[i])\n",
    "    \n",
    "embeddings_test = []\n",
    "for i in range(len(test_x)):\n",
    "    embeddings_test.append(test_x.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, gamma=6)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SVC(kernel='rbf', C=10, gamma = 6)\n",
    "model.fit(embeddings, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8647308781869688"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(embeddings_test,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_data(data_frame):\n",
    "    n_examples = len(data_frame)\n",
    "    train_examples = []#store input example\n",
    "    for i in range(n_examples):\n",
    "        example = data_frame.loc[i]\n",
    "        #sent_a  and sent_b with scores as label is added to train example\n",
    "        train_examples.append(InputExample(texts=[example['text']],label=example['target'].astype('float32')))\n",
    "    return train_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_predicts(data_type, trained_model):\n",
    "    predicts = []\n",
    "    for i in tqdm(range(len(data_type))):\n",
    "        #convert into embeddings \n",
    "        embeddings1 = np.array([trained_model.encode(data_type.loc[i]['text'],convert_to_numpy=True)])\n",
    "        #find cosine similarity\n",
    "    return predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    preprocessed = []\n",
    "    for i in range(len(examples)):\n",
    "        preprocessed.append(tokenizer(examples['text'].iloc[i], truncation='longest_first'))\n",
    "    examples\n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 8740, 16774, 2594, 1017, 2095, 2214, 2056, 2034, 6251, 2651, 2342, 8549, 2113, 2116, 2616, 2081, 5390, 2963, 2224, 2362, 2036, 3728, 2318, 10320, 2184, 2126, 2758, 20682, 1021, 15288, 3407, 5082, 2172, 19366, 2144, 4083, 10639, 12476, 2154, 3071, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(df['text'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = preprocess_function(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>autistic 3 year old said first sentence today ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>left side lips almost completely torn face dog...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dating two years proposed said yes engaged</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>one day time smiling makes big difference</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trying hardest happy today hope face brings so...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>world moves withouttoday whim decided google s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>call helped 23 colorado struggled since kid ne...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>wronghello know start try keep short clear und...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>give completelymany years trying success decid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>drivenothing makes want get morning feel nothi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  target\n",
       "0    autistic 3 year old said first sentence today ...       0\n",
       "1    left side lips almost completely torn face dog...       0\n",
       "2           dating two years proposed said yes engaged       0\n",
       "3            one day time smiling makes big difference       0\n",
       "4    trying hardest happy today hope face brings so...       0\n",
       "..                                                 ...     ...\n",
       "995  world moves withouttoday whim decided google s...       1\n",
       "996  call helped 23 colorado struggled since kid ne...       1\n",
       "997  wronghello know start try keep short clear und...       1\n",
       "998  give completelymany years trying success decid...       1\n",
       "999  drivenothing makes want get morning feel nothi...       1\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "051f1d82aaa443d48f9f96d18d62a689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 126\n",
      "  Number of trainable parameters = 66955010\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,attention_mask.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-253-39dc3ba9f4f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m )\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1503\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1504\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1505\u001b[1;33m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1506\u001b[0m         )\n\u001b[0;32m   1507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1747\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1748\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1749\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1751\u001b[0m                 if (\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2506\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2507\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2508\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2509\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2510\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2552\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"loss\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2553\u001b[0m                 raise ValueError(\n\u001b[1;32m-> 2554\u001b[1;33m                     \u001b[1;34m\"The model did not return a loss from the inputs, only the following keys: \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2555\u001b[0m                     \u001b[1;34mf\"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2556\u001b[0m                 )\n",
      "\u001b[1;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,attention_mask."
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_model\",\n",
    "    learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=16,\n",
    "#     per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "#     load_best_model_at_end=True,\n",
    "#     push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=preprocess_function(df[:1000]),\n",
    "    eval_dataset=preprocess_function(df[1000:]),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 17712, 7377, 2527, 22525, 102], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"akshara nair\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid.\n",
      "Your token has been saved in your configured git credential helpers (manager).\n",
      "Your token has been saved to C:\\Users\\Ammu\\.huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
